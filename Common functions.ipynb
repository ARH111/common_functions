{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg(Y, X):\n",
    "    import numpy as np\n",
    "    import statsmodels.api as sm\n",
    "    X_with_constant = sm.add_constant(X)\n",
    "    bf_lm = sm.OLS(Y, X_with_constant)\n",
    "    bf_lm_fit = bf_lm.fit()\n",
    "    summary = bf_lm_fit.summary()\n",
    "    display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cronbach's aplha if item removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cronbach_alpha_test() -- returns just cronbach's alpha (raw alpha)\n",
    "# cronbach_alpha() -- returns raw alpha plus alpha if item removed, item mean and StdDev\n",
    " \n",
    "import pandas as pd\n",
    "\n",
    "def cronbach_alpha_test(items):\n",
    "    items = pd.DataFrame(items)\n",
    "    items_count = items.shape[1]\n",
    "    variance_sum = float(items.var(axis=0, ddof=1).sum())\n",
    "    total_var = float(items.sum(axis=1).var(ddof=1))\n",
    "    return (items_count / float(items_count - 1) *\n",
    "            (1 - variance_sum / total_var))\n",
    "\n",
    "def cronbach_alpha(df):\n",
    "    # Print raw alpha\n",
    "    alpha = round(cronbach_alpha_test(df), 2)\n",
    "    print(\"Raw alpha: \", alpha)\n",
    "    # Calc mean and StdDev of items\n",
    "    mean_list = [df.mean(axis = 0)]\n",
    "    std_list = [df.std(axis = 0)]\n",
    "    # Calculate alpha if item removed and print final results\n",
    "    a_list = []\n",
    "    for n in df:\n",
    "        a_list.append(n)\n",
    "    for n in range(len(a_list)):\n",
    "        del_item = a_list[n]\n",
    "        removed_alpha_list = [x for x in a_list if x not in del_item]\n",
    "        alpha_test = df[removed_alpha_list]\n",
    "        a = round(cronbach_alpha_test(alpha_test), 2)\n",
    "        print(\"Raw alpha if {} removed: \".format(del_item), \"{:.2f}\".format(a),\n",
    "              \"    Item mean: \", \"{:.2f}\".format(mean_list[0][n]), \n",
    "              \"    Item StdDev: \", \"{:.2f}\".format(std_list[0][n]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_loop(Y, X):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import xgboost\n",
    "    from sklearn import model_selection\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    import statistics\n",
    "    accuracy_list = []\n",
    "    for item in range(20):\n",
    "        test_size = 0.33\n",
    "        X_train, X_test, y_train, y_test = model_selection.train_test_split(X, Y, test_size=test_size)\n",
    "        # fit model to training data\n",
    "        model = xgboost.XGBClassifier()\n",
    "        model.fit(X_train, np.ravel(y_train))\n",
    "        # make predictions for test data\n",
    "        y_pred = model.predict(X_test)\n",
    "        predictions = [round(value) for value in y_pred]\n",
    "        # evaluate predictions\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        accuracy_list.append(accuracy)    \n",
    "    model_accuracy = statistics.mean(accuracy_list)    \n",
    "    print(\"Accuracy:\", round(model_accuracy * 100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StatsModels logistic regression user defined function\n",
    "\n",
    "def logreg(y, x):\n",
    "    import numpy as np\n",
    "    import statsmodels.api as sm\n",
    "    x = sm.add_constant(x)\n",
    "    model = sm.Logit(y, x)\n",
    "    result = model.fit(method='newton')\n",
    "    diagonal_sum = result.pred_table().diagonal().sum()\n",
    "    predtable_sum = result.pred_table().sum()\n",
    "    accuracy = round(diagonal_sum / predtable_sum * 100, 3)\n",
    "    print(\"Model accuracy: \", str(accuracy) + \"%\")\n",
    "    print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_NaiveBayes(dat, classifier):\n",
    "    from sklearn import datasets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn import metrics\n",
    "    import statistics\n",
    "    \n",
    "    gnb = GaussianNB()\n",
    "    accuracy_list = []\n",
    "    \n",
    "    for item in range(20):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(dat, classifier, test_size = 0.2)  \n",
    "        gnb.fit(x_train, y_train)\n",
    "        y_pred = gnb.predict(x_test)\n",
    "        accuracy_score = metrics.accuracy_score(y_test, y_pred)\n",
    "        accuracy_list.append(accuracy_score)\n",
    "    \n",
    "    model_accuracy = round(statistics.mean(accuracy_list), 4)\n",
    "\n",
    "    print(\"Accuracy:\", model_accuracy * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condense code for barplot labels\n",
    "\n",
    "def add_value_labels(ax, spacing=5):\n",
    "    for rect in ax.patches:\n",
    "        y_value = rect.get_height()\n",
    "        x_value = rect.get_x() + rect.get_width() / 2\n",
    "        space = spacing\n",
    "        va = 'bottom'\n",
    "        if y_value < 0:\n",
    "            space *= -1\n",
    "            va = 'top'\n",
    "        label = \"{:.2f}\".format(y_value)\n",
    "        ax.annotate(\n",
    "            label,                      \n",
    "            (x_value, y_value),         \n",
    "            xytext=(0, space),          \n",
    "            textcoords=\"offset points\", \n",
    "            ha='center',                \n",
    "            va=va)  \n",
    "\n",
    "def analyse_corpus_sentiment(text):\n",
    "    import spacy\n",
    "    from vaderSentiment import vaderSentiment\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    english = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    negative = []\n",
    "    neutral = []\n",
    "    positive = []\n",
    "    compound = []\n",
    "    \n",
    "    for t in range(len(text)):\n",
    "        result = english(text[t])\n",
    "        sentences = [str(s) for s in result.sents]\n",
    "        analyzer = vaderSentiment.SentimentIntensityAnalyzer()\n",
    "        sentiment = [analyzer.polarity_scores(str(s)) for s in sentences]\n",
    "        \n",
    "        negative.append(sentiment[0]['neg'])\n",
    "        neutral.append(sentiment[0]['neu'])\n",
    "        positive.append(sentiment[0]['pos'])\n",
    "        compound.append(sentiment[0]['compound'])\n",
    "    \n",
    "    sentiment_score = pd.DataFrame()\n",
    "    sentiment_score['Negative'] = [round(np.mean(negative), 2)]\n",
    "    sentiment_score['Neutral'] = [round(np.mean(neutral), 2)]\n",
    "    sentiment_score['Positive'] = [round(np.mean(positive), 2)]\n",
    "    sentiment_score['Compound'] = [round(np.mean(compound), 2)]\n",
    "    \n",
    "    ax = sentiment_score.plot(kind = 'bar')\n",
    "    ax.set_xlabel(\"Sentiment\")\n",
    "    plt.grid(True)\n",
    "    plt.ylim([0, 1])\n",
    "    add_value_labels(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score as a list\n",
    "\n",
    "def analyse_corpus_sentiment(text):\n",
    "    import spacy\n",
    "    from vaderSentiment import vaderSentiment\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    english = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    negative = []\n",
    "    neutral = []\n",
    "    positive = []\n",
    "    compound = []\n",
    "    \n",
    "    for t in range(len(text)):\n",
    "        result = english(text[t])\n",
    "        sentences = [str(s) for s in result.sents]\n",
    "        analyzer = vaderSentiment.SentimentIntensityAnalyzer()\n",
    "        sentiment = [analyzer.polarity_scores(str(s)) for s in sentences]\n",
    "        \n",
    "        negative.append(sentiment[0]['neg'])\n",
    "        neutral.append(sentiment[0]['neu'])\n",
    "        positive.append(sentiment[0]['pos'])\n",
    "        compound.append(sentiment[0]['compound'])\n",
    "    \n",
    "    sentiment_score = pd.DataFrame()\n",
    "    sentiment_score['Negative'] = [round(np.mean(negative), 2)]\n",
    "    sentiment_score['Neutral'] = [round(np.mean(neutral), 2)]\n",
    "    sentiment_score['Positive'] = [round(np.mean(positive), 2)]\n",
    "    sentiment_score['Compound'] = [round(np.mean(compound), 2)]\n",
    "    \n",
    "    display(sentiment_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
